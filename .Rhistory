}
generate_and_export_data(gen_model,"Sleeping",map_seqs2char)
library(keras)
library(kerasR)
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = checkpoint_path,
save_weights_only = TRUE,
save_best_only = TRUE,
verbose = 1
)
build_model_simple_rnn <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
kernel_regularizer = regularizer_l2(0.001),
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
kernel_regularizer = regularizer_l2(0.001),
recurrent_initializer='glorot_uniform',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation='sigmoid')
return(model)
}
build_model_lstm <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
layer_dropout(rate = 0.5)%>%
layer_lstm(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
recurrent_activation='sigmoid',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation = "sigmoid")
return(model)
}
compile_and_train <- function(model,no_epochs,size_batch,learning_rate,validtion,input_train, output_train){
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
history <- model %>% fit(
input_train, output_train,
epochs = no_epochs,
batch_size = size_batch,
callbacks = list(cp_callback), # pass callback to training,
verbose = 2,
validation_split = validtion
)
model %>% save_model_tf("training_model")
}
generate_and_export_data<-function(gen_model,seed_word,map_seqs2char){
input_eval <- list(map_seqs2char$seed_word)
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
# new_model %>% reset_states()
# new_model %>% save_model_tf("generator_model")
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
write.csv(text_generated, "data_gen.csv")
write.table(text_generated, "data_gen.txt")
}
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,167,310)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor, 100,4000)
input_train <- training_set$x
output_train <- training_set$y
batch <- 100
emb_dim <- 39
rnn_u <- 32
learning_rate<-0.01
epochs <- 30
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
compile_and_train(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
plot(history)
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,167,310)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor, 100,4000)
input_train <- training_set$x
output_train <- training_set$y
batch <- 100
emb_dim <- 39
rnn_u <- 32
learning_rate<-0.01
epochs <- 30
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
compile_and_train(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
plot(history)
plot(history)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% keras$Model$build(tf$TensorShape(c(1L,NULL)))
# gen_model %>% compile(
#   optimizer=optimizer_adam(learning_rate = learning_rate),
#   loss = "sparse_categorical_crossentropy",
#   metrics = c("acc")
# )
gen_model %>% save_model_tf("generator_model")
summary(gen_model)
gen_model %>% reset_states()
seed_word<-"Sleeping"
generate_and_export_data(gen_model,seed_word,map_seqs2char)
library(keras)
library(kerasR)
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = checkpoint_path,
save_weights_only = TRUE,
save_best_only = TRUE,
verbose = 1
)
build_model_simple_rnn <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
kernel_regularizer = regularizer_l2(0.001),
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
kernel_regularizer = regularizer_l2(0.001),
recurrent_initializer='glorot_uniform',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation='sigmoid')
return(model)
}
build_model_lstm <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
layer_dropout(rate = 0.5)%>%
layer_lstm(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
recurrent_activation='sigmoid',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation = "sigmoid")
return(model)
}
compile_and_train <- function(model,no_epochs,size_batch,learning_rate,validtion,input_train, output_train){
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
history <- model %>% fit(
input_train, output_train,
epochs = no_epochs,
batch_size = size_batch,
callbacks = list(cp_callback), # pass callback to training,
verbose = 2,
validation_split = validtion
)
model %>% save_model_tf("training_model")
}
seed_word<-"Sleeping"
input_eval <- list(map_seqs2char$seed_word)
input_eval <- tf$expand_dims(input_eval,0L)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,167,310)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor, 100,4000)
input_train <- training_set$x
output_train <- training_set$y
batch <- 100
emb_dim <- 39
rnn_u <- 32
learning_rate<-0.01
epochs <- 30
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
compile_and_train(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
plot(history)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% keras$Model$build(tf$TensorShape(c(1L,NULL)))
gen_model %>% reset_states()
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% save_model_tf("generator_model")
summary(gen_model)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,167,310)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor, 100,4000)
input_train <- training_set$x
output_train <- training_set$y
batch <- 100
emb_dim <- 39
rnn_u <- 32
learning_rate<-0.01
epochs <- 30
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
compile_and_train(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
plot(history)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% keras$Model$build(tf$TensorShape(c(1L,NULL)))
gen_model %>% reset_states()
# gen_model %>% compile(
#   optimizer=optimizer_adam(learning_rate = learning_rate),
#   loss = "sparse_categorical_crossentropy",
#   metrics = c("acc")
# )
gen_model %>% save_model_tf("generator_model")
summary(gen_model)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% keras$Model$build(tf$TensorShape(c(1L,NULL)))
gen_model %>% reset_states()
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% save_model_tf("generator_model")
summary(gen_model)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% keras$Model$build(tf$TensorShape(c(1L,NULL)))
gen_model %>% reset_states()
gen_model %>% save_model_tf("generator_model")
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% save_model_tf("generator_model")
summary(gen_model)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
plot(history)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% save_model_tf("generator_model")
summary(gen_model)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
for (i in 2:40) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
gen_model %>% keras$Model$build(tf$TensorShape(c(1L,NULL)))
t1<-input_train
t2<-output_train
fresh_model %>% evaluate(t1, t2, verbose = 0)
gen_model %>% evaluate(t1, t2, verbose = 0)
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% evaluate(t1, t2, verbose = 0)
summary(gen_model)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
t1<-input_train
t2<-output_train
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% evaluate(t1, t2, verbose = 0)
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(output_eval,0L)
output_eval <- tf$expand_dims(output_eval,0L)
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(output_eval,0L)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(output_eval,0L)
input_eval
input_eval <- list(map_seqs2char$"Sleeping")
input_eval
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(24,0L)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(24,0L)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(24,0L)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(24,0L)
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(24,0L)
gen_model %>% compile()
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
summary(gen_model)
plot(history)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(24,0L)
gen_model %>% compile()
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
summary(gen_model)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- tf$expand_dims(input_eval,0L)
output_eval <- tf$expand_dims(24,0L)
gen_model %>% keras$Model$build(tf$TensorShape(c(1L,NULL)))
gen_model %>% reset_states()
gen_model %>% evaluate(input_eval, output_eval, verbose = 0)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
gen_model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% save_model_tf("generator_model")
