training_set <- get_batch(dataSet2tensor, 144,4360)
input_train <- training_set$x
output_train <- training_set$y
batch <- 100
emb_dim <- 39
rnn_u <- 50
learning_rate<-0.01
epochs <- 80
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,167,310)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor, 144,4360)
input_train <- training_set$x
output_train <- training_set$y
batch <- 100
emb_dim <- 39
rnn_u <- 50
learning_rate<-0.01
epochs <- 80
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
compile_and_train(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
plot(history)
library(keras)
library(kerasR)
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = checkpoint_path,
save_weights_only = TRUE,
save_best_only = TRUE,
verbose = 1
)
build_model_simple_rnn <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
kernel_regularizer = regularizer_l2(0.001),
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
kernel_regularizer = regularizer_l2(0.001),
recurrent_initializer='glorot_uniform',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation='sigmoid')
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
return(model)
}
build_model_lstm <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
# layer_dropout(rate = 0.5)%>%
layer_lstm(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
recurrent_activation='sigmoid',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation = "softmax")
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
return(model)
}
compile_and_train <- function(model,no_epochs,size_batch,learning_rate,validtion,input_train, output_train){
history <- model %>% fit(
input_train, output_train,
epochs = no_epochs,
batch_size = size_batch,
callbacks = list(cp_callback), # pass callback to training,
verbose = 2,
validation_split = validtion
)
model %>% save_model_tf("training_model")
}
library(keras)
library(kerasR)
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = checkpoint_path,
save_weights_only = TRUE,
save_best_only = TRUE,
verbose = 1
)
build_model_simple_rnn <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
kernel_regularizer = regularizer_l2(0.001),
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
kernel_regularizer = regularizer_l2(0.001),
recurrent_initializer='glorot_uniform',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation='sigmoid')
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
return(model)
}
build_model_lstm <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
# layer_dropout(rate = 0.5)%>%
layer_lstm(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
recurrent_activation='sigmoid',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation = "softmax")
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
return(model)
}
compile_and_train <- function(model,no_epochs,size_batch,learning_rate,validtion,input_train, output_train){
history <- model %>% fit(
input_train, output_train,
epochs = no_epochs,
batch_size = size_batch,
callbacks = list(cp_callback), # pass callback to training,
verbose = 2,
validation_split = validtion
)
model %>% save_model_tf("training_model")
}
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor, 100,4360)
input_train <- training_set$x
output_train <- training_set$y
batch <- 100
emb_dim <- 39
rnn_u <- 50
learning_rate<-0.01
epochs <- 80
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
library(keras)
library(kerasR)
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = checkpoint_path,
save_weights_only = TRUE,
save_best_only = TRUE,
verbose = 1
)
build_model_simple_rnn <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
kernel_regularizer = regularizer_l2(0.001),
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_simple_rnn(
units = rnn_units,
return_sequences=TRUE,
kernel_regularizer = regularizer_l2(0.001),
recurrent_initializer='glorot_uniform',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation='sigmoid')
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
return(model)
}
build_model_lstm <- function(vocab_size, embedding_dim, rnn_units, batch_size){
model <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
batch_size = batch_size) %>%
# layer_dropout(rate = 0.5)%>%
layer_lstm(
units = rnn_units,
return_sequences=TRUE,
recurrent_initializer='glorot_uniform',
recurrent_activation='sigmoid',
stateful = TRUE
) %>%
layer_dropout(rate = 0.5)%>%
layer_dense(vocab_size,activation = "softmax")
model %>% compile(
optimizer=optimizer_adam(learning_rate = learning_rate),
loss = "sparse_categorical_crossentropy",
metrics = c("acc")
)
return(model)
}
train <- function(model,no_epochs,size_batch,learning_rate,validtion,input_train, output_train){
history <- model %>% fit(
input_train, output_train,
epochs = no_epochs,
batch_size = size_batch,
callbacks = list(cp_callback), # pass callback to training,
verbose = 2,
validation_split = validtion
)
model %>% save_model_tf("training_model")
}
train(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
trainM <-  load_model_hdf5("training_model.h5")
summary(trainM)
library(keras)
trainM <-  load_model_hdf5("training_model.h5")
summary(trainM)
gen <-  load_model_hdf5("generator_model.h5")
gen <-  load_model_hdf5("generator_model.h5")
summary(gen)
input_eval <- list(map_seqs2char$Sleeping)
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
size <- 30
for (i in 2:size) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
library(tensorflow)
trainM <-  load_model_hdf5("training_model.h5")
summary(trainM)
gen <-  load_model_hdf5("generator_model.h5")
summary(gen)
input_eval <- list(map_seqs2char$Sleeping)
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
size <- 30
for (i in 2:size) {
predictions <- gen_model(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
gen <-  load_model_hdf5("generator_model.h5")
summary(gen)
gen <-  load_model_hdf5("generator_model.h5")
summary(gen)
input_eval <- list(map_seqs2char$Sleeping)
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
size <- 30
for (i in 2:size) {
predictions <- gen(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
db <- file.choose()
dataset <- read.spss(db, to.data.frame=TRUE)
data(dataset)
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,167,310)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor, 100,4360)
input_train <- training_set$x
output_train <- training_set$y
batch <- 4
emb_dim <- 64
rnn_u <- 32
learning_rate<-0.0001
epochs <- 10
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
db <- file.choose()
dataset <- read.spss(db, to.data.frame=TRUE)
data(dataset)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
dataset <- read.spss(db, to.data.frame=TRUE)
library(foreign)
db <- file.choose()
dataset <- read.spss(db, to.data.frame=TRUE)
data(dataset)
dataset
db <- file.choose()
# db <- file.choose()
dataset <- read.spss("uk_4wave_caddi_data.sav", to.data.frame = TRUE)
data(dataset)
db <- file.choose()
db
dataset <- read.spss(db, to.data.frame = TRUE)
gc()
gc()
library(haven)
dataset <- read_sav("uk_4wave_caddi_data.sav")
View(dataset)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
library(foreign)
library(TraMineR)
fix_seq<-function(seq){
fseq <-gsub("  ","",seq)
if (substring(y,nchar(fseq),nchar(fseq)) == " "){
fseq <- substr(fseq,1,nchar(fseq)-1)
}
return(fseq)
}
convert_dataSet<-function(data_set, start_cols, stop_cols){
cols <- (stop_cols + 1) - start_cols
fixed_dataset<-matrix(nrow = nrow(data_set), ncol = cols)
for (i in 1:nrow(data_set)) {
for (j in 1:cols) {
seq <- as.character(data_set[i,(start_cols-1)+j])
fixed_dataset[i,j] <- fix_seq(seq)
}
}
return(fixed_dataset)
}
clean_data<-function(seq_opts){
neq_seq_opts<-c()
for (i in 1:length(seq_opts)) {
seq_opts[i] <- fix_seq(seq_opts[i])
}
return(seq_opts)
}
map_seq2index<-function(data_set,start_idx,stop_idx){
seq2index<-list()
seqs<-seqstatl(dataset[, start_idx:stop_idx])
clean_seqs<-clean_data(seqs)
for (seq in clean_seqs) {
if (!seq %in% names(seq2index)){
seq2index[[seq]]<-length(seq2index) + 1
}
}
return(seq2index)
}
make_tensor<-function(map, data_set){
rows = nrow(data_set)
cols = ncol(data_set)
tensor <- matrix(nrow = rows,ncol = cols)
for (i in 1:rows) {
for (j in 1:cols) {
tensor[i,j] <- map[[data_set[i,j]]]
}
}
return(tensor)
}
get_batch<-function(vectorized_data, seq_legth, batch_size){
size_of_seq <- ncol(vectorized_data)
selected_seq <- sample.int(nrow(vectorized_data), batch_size,replace = TRUE)
st_indx_in_seq <- sample.int(size_of_seq - seq_legth, batch_size,replace = TRUE)
x_batch <- matrix(nrow = batch_size, ncol = seq_legth)
y_batch <- matrix(nrow = batch_size, ncol = seq_legth)
for (i in 1:batch_size) {
st <- st_indx_in_seq[i]
x_batch[i,] <- vectorized_data[selected_seq[i], st:(st + seq_legth - 1)]
y_batch[i,] <- vectorized_data[selected_seq[i], (st+1):(st + seq_legth)]
}
return(list("x" = x_batch, "y" = y_batch))
}
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
library(foreign)
library(TraMineR)
fix_seq<-function(seq){
fseq <-gsub("  ","",seq)
if (substring(fseq,nchar(fseq),nchar(fseq)) == " "){
fseq <- substr(fseq,1,nchar(fseq)-1)
}
return(fseq)
}
convert_dataSet<-function(data_set, start_cols, stop_cols){
cols <- (stop_cols + 1) - start_cols
fixed_dataset<-matrix(nrow = nrow(data_set), ncol = cols)
for (i in 1:nrow(data_set)) {
for (j in 1:cols) {
seq <- as.character(data_set[i,(start_cols-1)+j])
fixed_dataset[i,j] <- fix_seq(seq)
}
}
return(fixed_dataset)
}
clean_data<-function(seq_opts){
neq_seq_opts<-c()
for (i in 1:length(seq_opts)) {
seq_opts[i] <- fix_seq(seq_opts[i])
}
return(seq_opts)
}
map_seq2index<-function(data_set,start_idx,stop_idx){
seq2index<-list()
seqs<-seqstatl(dataset[, start_idx:stop_idx])
clean_seqs<-clean_data(seqs)
for (seq in clean_seqs) {
if (!seq %in% names(seq2index)){
seq2index[[seq]]<-length(seq2index) + 1
}
}
return(seq2index)
}
make_tensor<-function(map, data_set){
rows = nrow(data_set)
cols = ncol(data_set)
tensor <- matrix(nrow = rows,ncol = cols)
for (i in 1:rows) {
for (j in 1:cols) {
tensor[i,j] <- map[[data_set[i,j]]]
}
}
return(tensor)
}
get_batch<-function(vectorized_data, seq_legth, batch_size){
size_of_seq <- ncol(vectorized_data)
selected_seq <- sample.int(nrow(vectorized_data), batch_size,replace = TRUE)
st_indx_in_seq <- sample.int(size_of_seq - seq_legth, batch_size,replace = TRUE)
x_batch <- matrix(nrow = batch_size, ncol = seq_legth)
y_batch <- matrix(nrow = batch_size, ncol = seq_legth)
for (i in 1:batch_size) {
st <- st_indx_in_seq[i]
x_batch[i,] <- vectorized_data[selected_seq[i], st:(st + seq_legth - 1)]
y_batch[i,] <- vectorized_data[selected_seq[i], (st+1):(st + seq_legth)]
}
return(list("x" = x_batch, "y" = y_batch))
}
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
vocab
map_seqs2char
f_dataSet <- convert_dataSet(dataset,167,310)
f_dataSet
View(dataset)
m<-read.spss("uk_4wave_caddi_data.sav", to.data.frame = TRUE)
View(m)
View(dataset)
dataset<-read.spss("uk_4wave_caddi_data.sav", to.data.frame = TRUE)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = 167,stop_idx = 310)
vocab <- tail(map_seqs2char,1)[[1]] + 1
map_seqs2char
f_dataSet <- convert_dataSet(dataset,167,310)
gc()
gc()
