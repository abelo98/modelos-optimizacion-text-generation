gen <- load_model_hdf5("generator_model.h5")
gen_model %>% save_model_hdf5("generator_model.h5",include_optimizer = FALSE)
summary(gen_model)
gen <- load_model_hdf5("generator_model.h5")
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% reset_states()
gen_model %>% save_model_hdf5("generator_model.h5",include_optimizer = FALSE)
summary(gen_model)
gen <- load_model_hdf5("generator_model.h5")
summary(gen)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% reset_states()
gen_model %>% save_model_hdf5("generator_model.h5",compile=FALSE)
gen_model %>% save_model_hdf5("generator_model.h5")
summary(gen_model)
gen <- load_model_hdf5("generator_model.h5",compile=FALSE)
summary(gen)
library(keras)
library(tensorflow)
gen <-  load_model_hdf5("generator_model.h5",compile=FALSE)
summary(gen)
input_eval <- list(map_seqs2char$Sleeping)
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_word)
size <- 30
for (i in 2:size) {
predictions <- gen(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
text_generated <- list("Sleeping")
size <- 30
for (i in 2:size) {
predictions <- gen(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
training_set <- get_batch(dataSet2tensor,100 ,4000)
input_train <- training_set$x
output_train <- training_set$y
batch <- 32
emb_dim <- 64
rnn_u <- 64
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
View(dataset)
myfunt <- function(){
cat("Enter an integer or whole number : \n")
enter <- as.integer(readline(prompt = ""))
cat("You sumitted : \n"); str(enter)
}
10
myfunt()
myfunt()
myfunt <- function(){
db <- file.choose()
dataset<-read.spss(db, to.data.frame = TRUE)
cat("Enter an integer or whole number : \n")
enter <- as.integer(readline(prompt = ""))
cat("You sumitted : \n"); str(enter)
}
myfunt()
initialize_dataSet
initialize_dataSet()
initialize_dataSet <- function(){
db <- file.choose()
dataset<-read.spss(db, to.data.frame = TRUE)
cat("Enter start column of the activities in dataset : \n")
start_i <- as.integer(readline(prompt = ""))
cat("Enter last column of the activities in dataset : \n")
stop_i <- as.integer(readline(prompt = ""))
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = start_i,stop_idx = stop_i)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,start_i,stop_i)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
}
initialize_dataSet()
source("utils.R")
source("network_architecture.R")
library(tensorflow)
library(keras)
library(foreign)
initialize_dataSet()
initialize_dataSet()
start_i <- 167
stop_i <- 310
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = start_i,stop_idx = stop_i)
db <- file.choose()
dataset<-read.spss(db, to.data.frame = TRUE)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = start_i,stop_idx = stop_i)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,start_i,stop_i)
library(foreign)
library(TraMineR)
fix_seq<-function(seq){
fseq <-gsub("  ","",seq)
if (substring(fseq,nchar(fseq),nchar(fseq)) == " "){
fseq <- substr(fseq,1,nchar(fseq)-1)
}
return(fseq)
}
convert_dataSet<-function(data_set, start_cols, stop_cols){
cols <- (stop_cols + 1) - start_cols
fixed_dataset<-matrix(nrow = nrow(data_set), ncol = cols)
for (i in 1:nrow(data_set)) {
for (j in 1:cols) {
seq <- as.character(data_set[i,(start_cols-1)+j])
fixed_dataset[i,j] <- fix_seq(seq)
}
}
return(fixed_dataset)
}
clean_data<-function(seq_opts){
neq_seq_opts<-c()
for (i in 1:length(seq_opts)) {
seq_opts[i] <- fix_seq(seq_opts[i])
}
return(seq_opts)
}
map_seq2index<-function(data_set,start_idx,stop_idx){
seq2index<-list()
seqs<-seqstatl(dataset[, start_idx:stop_idx])
clean_seqs<-clean_data(seqs)
for (seq in clean_seqs) {
if (!seq %in% names(seq2index)){
seq2index[[seq]]<-length(seq2index) + 1
}
}
return(seq2index)
}
# make_tensor<-function(map, data_set){
#   rows = nrow(data_set)
#   cols = ncol(data_set)
#   tensor <- matrix(nrow = rows,ncol = cols)
#
#   for (i in 1:rows) {
#     for (j in 1:cols) {
#       tensor[i,j] <- map[[data_set[i,j]]]
#     }
#   }
#   return(tensor)
# }
get_batch<-function(vectorized_data, seq_legth, batch_size){
size_of_seq <- ncol(vectorized_data)
selected_seq <- sample.int(nrow(vectorized_data), batch_size,replace = TRUE)
st_indx_in_seq <- sample.int(size_of_seq - seq_legth, batch_size,replace = TRUE)
x_batch <- matrix(nrow = batch_size, ncol = seq_legth)
y_batch <- matrix(nrow = batch_size, ncol = seq_legth)
for (i in 1:batch_size) {
st <- st_indx_in_seq[i]
x_batch[i,] <- vectorized_data[selected_seq[i], st:(st + seq_legth - 1)]
y_batch[i,] <- vectorized_data[selected_seq[i], (st+1):(st + seq_legth)]
}
return(list("x" = x_batch, "y" = y_batch))
}
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = start_i,stop_idx = stop_i)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,start_i,stop_i)
library(foreign)
library(TraMineR)
fix_seq<-function(seq){
fseq <-gsub("  ","",seq)
if (substring(fseq,nchar(fseq),nchar(fseq)) == " "){
fseq <- substr(fseq,1,nchar(fseq)-1)
}
return(fseq)
}
convert_dataSet<-function(data_set, start_cols, stop_cols){
cols <- (stop_cols + 1) - start_cols
fixed_dataset<-matrix(nrow = nrow(data_set), ncol = cols)
for (i in 1:nrow(data_set)) {
for (j in 1:cols) {
seq <- as.character(data_set[i,(start_cols-1)+j])
fixed_dataset[i,j] <- fix_seq(seq)
}
}
return(fixed_dataset)
}
clean_data<-function(seq_opts){
neq_seq_opts<-c()
for (i in 1:length(seq_opts)) {
seq_opts[i] <- fix_seq(seq_opts[i])
}
return(seq_opts)
}
map_seq2index<-function(data_set,start_idx,stop_idx){
seq2index<-list()
seqs<-seqstatl(dataset[, start_idx:stop_idx])
clean_seqs<-clean_data(seqs)
for (seq in clean_seqs) {
if (!seq %in% names(seq2index)){
seq2index[[seq]]<-length(seq2index) + 1
}
}
return(seq2index)
}
make_tensor<-function(map, data_set){
rows = nrow(data_set)
cols = ncol(data_set)
tensor <- matrix(nrow = rows,ncol = cols)
for (i in 1:rows) {
for (j in 1:cols) {
tensor[i,j] <- map[[data_set[i,j]]]
}
}
return(tensor)
}
get_batch<-function(vectorized_data, seq_legth, batch_size){
size_of_seq <- ncol(vectorized_data)
selected_seq <- sample.int(nrow(vectorized_data), batch_size,replace = TRUE)
st_indx_in_seq <- sample.int(size_of_seq - seq_legth, batch_size,replace = TRUE)
x_batch <- matrix(nrow = batch_size, ncol = seq_legth)
y_batch <- matrix(nrow = batch_size, ncol = seq_legth)
for (i in 1:batch_size) {
st <- st_indx_in_seq[i]
x_batch[i,] <- vectorized_data[selected_seq[i], st:(st + seq_legth - 1)]
y_batch[i,] <- vectorized_data[selected_seq[i], (st+1):(st + seq_legth)]
}
return(list("x" = x_batch, "y" = y_batch))
}
library(foreign)
library(TraMineR)
fix_seq<-function(seq){
fseq <-gsub("  ","",seq)
if (substring(fseq,nchar(fseq),nchar(fseq)) == " "){
fseq <- substr(fseq,1,nchar(fseq)-1)
}
return(fseq)
}
convert_dataSet<-function(data_set, start_cols, stop_cols){
cols <- (stop_cols + 1) - start_cols
fixed_dataset<-matrix(nrow = nrow(data_set), ncol = cols)
for (i in 1:nrow(data_set)) {
for (j in 1:cols) {
seq <- as.character(data_set[i,(start_cols-1)+j])
fixed_dataset[i,j] <- fix_seq(seq)
}
}
return(fixed_dataset)
}
clean_data<-function(seq_opts){
neq_seq_opts<-c()
for (i in 1:length(seq_opts)) {
seq_opts[i] <- fix_seq(seq_opts[i])
}
return(seq_opts)
}
map_seq2index<-function(data_set,start_idx,stop_idx){
seq2index<-list()
seqs<-seqstatl(dataset[, start_idx:stop_idx])
clean_seqs<-clean_data(seqs)
for (seq in clean_seqs) {
if (!seq %in% names(seq2index)){
seq2index[[seq]]<-length(seq2index) + 1
}
}
return(seq2index)
}
make_tensor<-function(map, data_set){
rows = nrow(data_set)
cols = ncol(data_set)
tensor <- matrix(nrow = rows,ncol = cols)
for (i in 1:rows) {
for (j in 1:cols) {
tensor[i,j] <- map[[data_set[i,j]]]
}
}
return(tensor)
}
get_batch<-function(vectorized_data, seq_legth, batch_size){
size_of_seq <- ncol(vectorized_data)
selected_seq <- sample.int(nrow(vectorized_data), batch_size,replace = TRUE)
st_indx_in_seq <- sample.int(size_of_seq - seq_legth, batch_size,replace = TRUE)
x_batch <- matrix(nrow = batch_size, ncol = seq_legth)
y_batch <- matrix(nrow = batch_size, ncol = seq_legth)
for (i in 1:batch_size) {
st <- st_indx_in_seq[i]
x_batch[i,] <- vectorized_data[selected_seq[i], st:(st + seq_legth - 1)]
y_batch[i,] <- vectorized_data[selected_seq[i], (st+1):(st + seq_legth)]
}
return(list("x" = x_batch, "y" = y_batch))
}
total <- 4330
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor,seq_legth ,total)
seq_legth
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor,seq_legth ,total)
seq_length <- 100
total <- 4330
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor,seq_length ,total)
input_train <- training_set$x
output_train <- training_set$y
batch <- 32
emb_dim <- 64
rnn_u <- 64
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
total <- 4128
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor,seq_length ,total)
input_train <- training_set$x
output_train <- training_set$y
batch <- 32
emb_dim <- 64
rnn_u <- 64
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
batch <- 6
emb_dim <- 64
rnn_u <- 64
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
total <- 4360
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor,seq_length ,total)
input_train <- training_set$x
output_train <- training_set$y
batch <- 8
emb_dim <- 64
rnn_u <- 64
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
rnn_u <- 512
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
batch <- 32
emb_dim <- 64
rnn_u <- 16
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
input_eval <- list(map_seqs2char$"Sleeping")
input_eval <- list(map_seqs2char$s)
s
s<-"Sleeping"
input_eval <- list(map_seqs2char$s)
s
input_eval
input_eval <- list(map_seqs2char$"Sleeping")
input_eval
map_seqs2char[s]
map_seqs2char[s]
input_eval <- list(map_seqs2char[s])
input_eval
input_eval <- list(map_seqs2char[[s]])
input_eval
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list("Sleeping")
for (i in 2:size) {
predictions <- gen(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
size <- 30
for (i in 2:size) {
predictions <- gen(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
gen <-  load_model_hdf5("generator_model.h5",compile=FALSE)
batch <- 32
emb_dim <- 64
rnn_u <- 16
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
seq_length <- 100
total <- 4000
db <- file.choose()
dataset<-read.spss(db, to.data.frame = TRUE)
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = start_i,stop_idx = stop_i)
vocab <- tail(map_seqs2char,1)[[1]] + 1
f_dataSet <- convert_dataSet(dataset,start_i,stop_i)
dataSet2tensor <- make_tensor(map_seqs2char,f_dataSet)
training_set <- get_batch(dataSet2tensor,seq_length ,total)
input_train <- training_set$x
output_train <- training_set$y
batch <- 32
emb_dim <- 64
rnn_u <- 16
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
batch <- 32
emb_dim <- 64
rnn_u <- 64
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
batch <- 32
emb_dim <- 64
rnn_u <- 32
learning_rate<-0.0001
epochs <- 85
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
gen_model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=1)
gen_model %>% load_model_weights_tf(checkpoint_path)
gen_model %>% reset_states()
gen_model %>% save_model_hdf5("generator_model.h5")
summary(gen_model)
start_i <- 167
stop_i <- 310
size <- 30
seed_text<-"Sleeping"
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = start_i,stop_idx = stop_i)
input_eval <- list(map_seqs2char[[seed_text]])
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_text)
for (i in 2:size) {
predictions <- gen(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
gen <-  load_model_hdf5("generator_model.h5",compile=FALSE)
summary(gen)
start_i <- 167
stop_i <- 310
size <- 30
seed_text<-"Sleeping"
map_seqs2char<-map_seq2index(data_set = dataset,start_idx = start_i,stop_idx = stop_i)
input_eval <- list(map_seqs2char[[seed_text]])
input_eval <- tf$expand_dims(input_eval,0L)
text_generated <- list(seed_text)
for (i in 2:size) {
predictions <- gen(input_eval)
predictions < tf$squeeze(predictions,0L)
predicted_id <- tf$random$categorical(predictions[1,,], num_samples=1L)
input_eval<-predicted_id
converted_pred_id <- as.double(predicted_id[1,1]) + 1
text_generated[i]<-(names(map_seqs2char)[converted_pred_id])
}
print(text_generated)
batch <- 32
emb_dim <- 64
rnn_u <- 64
learning_rate<-0.0001
epochs <- 50
validation_perct<-0.2
checkpoint_path <- "checkpoints/cp.ckpt"
model<- build_model_lstm(vocab, embedding_dim=emb_dim, rnn_units=rnn_u, batch_size=batch)
summary(model)
train_model(model,epochs,batch,learning_rate,validation_perct,input_train,output_train)
